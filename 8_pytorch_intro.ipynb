{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNf+NWNcuuiTxB5OgoB7RBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoz/mlclass/blob/master/8_pytorch_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI6IcySOqTwL"
      },
      "source": [
        "import torch as tc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly_XpcwEqUsg"
      },
      "source": [
        "#PyTorch Start\n",
        "\n",
        "파이토치는 페이스북 AI 리서치 랩이 만들고 2016년 공개한 오픈소스 머신 라이브러리이다. 인공지능 라이브러리로서는 케라스, 텐서플로등에 비해 늦게 합류하였으나 상당히 빠른 속도와 함게 다양한 형태의 모델의 개발이 자유롭기 때문에 점점 사용자가 늘어나고 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ0g8a9cqXvv"
      },
      "source": [
        "## Tensor \n",
        "\n",
        "파이토치는 텐서라는 매트릭스 데이터를 기반으로 이뤄져있다는 점에서 Numpy 와 매우 유사하다.  하지만 GPU 연산을 지원한다는 점과 딥러닝을 위한 자동미분모듈을 사용할 수 있다는 점등에서 딥러닝용으로 차별화된다. 또한 텐서 연산을 이용한 저수준 모델 구성이 가능할 뿐만 아니라 부터 케라스와 유사한 다양한 표준 모델등이 지원된다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgWWp04WVMEp"
      },
      "source": [
        "### Tensor 와 Numpy\n",
        "\n",
        "그러면 이제 텐서의 선언을 한번 보자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS8JANYvqwO3"
      },
      "source": [
        "t = tc.FloatTensor([1,2,3])\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GRGR2N_y1hI"
      },
      "source": [
        "Numpy 를 연상시키는 익숙한 코드일 것이다. 물론 Numpy 의 리스트로도 만들 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLp2tCS9q0FO"
      },
      "source": [
        "import numpy as np\n",
        "n = np.array([[3,4],[5,6]])\n",
        "t  = tc.FloatTensor(n)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfDFUkXrzkdm"
      },
      "source": [
        "다음 코드는 Numpy 가 Torch 에 끼친 영향을 느낄 수 있다. size 라는 자체 배열의 정보를 제공하는 함수가 있지만 Numpy 와 같은 shape 도 사용이 가능하다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs5PfJD1q-8u"
      },
      "source": [
        "print( t.dim() )\n",
        "print( t.size() )\n",
        "print( t.shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEno_MSW1YFn"
      },
      "source": [
        "또한 로우 칼럼 연산도 Numpy 와 유사하게 지원한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Mn_clwrJRG"
      },
      "source": [
        "import torch as tc \n",
        "\n",
        "t1,t2 = tc.FloatTensor([[1,2,3,4]]),tc.FloatTensor([[1],[2],[3]])\n",
        "print(t1 * t2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLZattlZ4oe7"
      },
      "source": [
        "### mean,max,min, argmax\n",
        "\n",
        "Torch 는 Numpy에서 지원하는 함수와 같은 함수를 지원하는 경우가 많다. 아래 몇몇 연산을 한번 알아보자 먼저 평균을 구하는 함수이다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Pr4gRU1-gW"
      },
      "source": [
        "a = np.array([1,2,3])\n",
        "print(a.mean())\n",
        "\n",
        "t = tc.FloatTensor([1,2,3])\n",
        "print(t.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Ucpyiv2xsb"
      },
      "source": [
        "미묘한 차이를 발견할 수 있다. np는 그 결과가 일반 정수로 리턴되는데 tc 는 그 결과값도 텐서임을 알 수 있다. 이는 모든 계산이 gpu 상에서 연속적으로 이뤄져야하기 때문이다. \n",
        "\n",
        "또한 배열단위의 연산의 경우 Numpy 와 유사하게 기준축을 설정할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e88xgn8sJsu"
      },
      "source": [
        "t = t1 + t2\n",
        "print(t)\n",
        "print( t.mean() )\n",
        "print( t.mean(dim = 0) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkEu3upm3kJQ"
      },
      "source": [
        "min, max 는 최소 최대값을 구하는 함수이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkxpLKT4sY22"
      },
      "source": [
        "print( t )\n",
        "print( t.max() )\n",
        "print( t.min(dim = 0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbC1nabT4BKm"
      },
      "source": [
        "argmax, argmin 은 최대, 최소값의 인덱스를 알려준다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aIqsVqXtH4H"
      },
      "source": [
        "print( t.argmax() )\n",
        "print( t.argmin(dim = 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvyz0azH44iH"
      },
      "source": [
        "### view\n",
        "\n",
        "Numpy 에서 reshape 가 중요한 역할을 했던걸 기억할 것이다 Tensor 는 이와 유사한 역할을 하는 함수로 view 를 제공한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSXXnwTxtdPA"
      },
      "source": [
        "print(t)\n",
        "print( t.view([4,3]) )\n",
        "print( t.view(2,6) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7BhTXqty03"
      },
      "source": [
        "t.view([-1,2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvVNmDJI5AH2"
      },
      "source": [
        "### Squeeze , Unsqueeze\n",
        "\n",
        "의외로 종종 등장하는 함수로서 squeeze 와 Unsqueeze 라는 함수가 있다. Squeeze 는 축을 하나 없애는 역할을 하고 Unsueeze 는 축을 추가하는 역할을 한다. 물론 view 를 사용하는 방법도 있지만 이를 쓰는 경우도 있으니 알아두자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuHs8T2ZupBv"
      },
      "source": [
        "t = tc.IntTensor([1,2,3,4,5])\n",
        "t = t.view(1,1,5,1)\n",
        "print(t)\n",
        "\n",
        "\n",
        "print( t.squeeze().shape )\n",
        "print( t.squeeze(0).shape )\n",
        "print( t.squeeze(1).shape )\n",
        "print( t.squeeze(2).shape ) # 해당 차원의 크기가 1이 아니면 아무일도 일어나지 안는다\n",
        "print( t.squeeze(3).shape )\n",
        "print( t.squeeze(-1).shape ) # 제일 뒤 차원"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzs7OFGxt_3P"
      },
      "source": [
        "t = tc.IntTensor([1,2,3,4,5])\n",
        "print( t.unsqueeze(-1).shape  ) # -1 은 가장 뒤 차원을 의미\n",
        "print( t.unsqueeze(-1) )\n",
        "print()\n",
        "print( t.unsqueeze(0).shape  )\n",
        "print( t.unsqueeze(0) ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPX1mwT45LYq"
      },
      "source": [
        "### Cat, Stack\n",
        "\n",
        "두 데이터를 하나의 데이터로 합치는 방법으롤 cat 과 stack 이 있다. cat 은 numpy의 concatenate 와 마찬가지로 두 데이터를 합친다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYONAsG5zK1A"
      },
      "source": [
        "t1,t2 = tc.FloatTensor([1,2,3]),tc.FloatTensor([4,5,6])\n",
        "\n",
        "print( tc.cat([t1,t2]), \"\\n\" )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdCR3AqC8S3h"
      },
      "source": [
        "1개 이상의 축을 가진 배열이라면 디폴트값은 첫 축을 기준으로 수행된다.  특정 축을 지정할때는 Numpy와 유사하게 dim 옵션을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW9EboAH0Jdp"
      },
      "source": [
        "t1,t2 = tc.FloatTensor([[1,2,3]]),tc.FloatTensor([[4,5,6]])\n",
        "print( tc.cat([t1,t2]) )\n",
        "print( tc.cat([t1,t2], dim = 1) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFOqK_999rIK"
      },
      "source": [
        "stack 의 경우 cat 과 유사하지만 데이터를 합칠대 축을 추가한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "untrqLMZw_nS"
      },
      "source": [
        "\n",
        "t1,t2 = tc.FloatTensor([1,2,3]),tc.FloatTensor([4,5,6])\n",
        "\n",
        "print(tc.stack([t1,t2]) )\n",
        "print(tc.stack([t1,t2], dim = 1) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9cn6Qfr5QcB"
      },
      "source": [
        "### zeros_like, ones_like\n",
        "\n",
        "np.zeros(), np.ones() 와 유사하게 tc 에서도 0이나 1로 가득찬 텐서를 생성할 수 있다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGfu1FI-0xC"
      },
      "source": [
        "print( tc.zeros((2,2)) ) \n",
        "print( tc.ones(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_lFUzyh_ATd"
      },
      "source": [
        "하지만 특정 텐서와 같은 형태를 만들 수도 있는데 zeros_like, ones_like 이라는 함수가 있다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh8NgsTo2SvJ"
      },
      "source": [
        "print(t)\n",
        "\n",
        "print(tc.zeros_like(t))\n",
        "print(tc.ones_like(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNO4qP4-b4E"
      },
      "source": [
        "print( np.ones(4) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbKRCgftAri5"
      },
      "source": [
        "### inplace operation\n",
        "\n",
        "어떤 텐서에 특정 값을 더하는 연산은 t = t + 3 이라고 할 수도 있지만 이는 메모리상에 t+3 이라는 배열을 생성한 후에 이를 대처하는 과정을 갖는다. 텐서의 크기가 거대할 경우 이는 속도에 무시못할 영향을 미친다. 어떤 값을 더하거나 교체할 경우 gpu 상에서는 이런 과정 없이 현재의 텐서상에 바로 업데이트를 할 필요가 있다. 이를 inploace operation 이라고 한다. \n",
        "\n",
        "예를 들어 t라는 텐서에 대해 t.mul(2) 은 t * 2 의 배열을 새로 생성한다. 만일 이를 다른 곳에 대입하지 안으면 그 후에 소멸되고 만다. 하지만 t.mul_ 는 inplace 연산으로서 연산 자체가 t 의 값을 업데이트해준다.  이는 메모리나 속도상에 큰 이점이 있으므로 특정 텐서를 업데이트 할때는 이 연산을 사용하는데 익숙해져야한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym9N9bGh2j6n"
      },
      "source": [
        "print(t.mul(2) ) \n",
        "print(t)\n",
        "print() \n",
        "print(t.mul_(2))\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUUoRlr320qa"
      },
      "source": [
        "print(t)\n",
        "\n",
        "t.add_(1)\n",
        "print(t)\n",
        "\n",
        "t.sub_(10)\n",
        "print(t)\n",
        "\n",
        "t.div_(10)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrK6bpMh4Tk4"
      },
      "source": [
        "### Numpy <-> Torch\n",
        "\n",
        "torch 가 많은 numpy 의 기능을 제공하지만 그럼에도 그 목적이 다르기 때문에 보다 광범위한 numpy 의 연산을 사용해 초기 데이터를 생성할 수 있다. 이를 위해서는 numpy와 torch 사이의 데이터 변형이 필요할 수 있다.  \n",
        "\n",
        "아래는 numpy -> torch 로의 데이터 변환을 보여준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPYMXy2b3NpA"
      },
      "source": [
        "n = np.array([1,2,3,4,5], dtype = 'float')\n",
        "\n",
        "t = tc.from_numpy(n)\n",
        "print(n)\n",
        "print(t)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GvHPO4GDbgP"
      },
      "source": [
        "torch 를 numpy로 변환하는 경우 텐서에 .numpy() 메소드를 제공한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C570niZ4GMR"
      },
      "source": [
        "print(t.numpy())\n",
        "print( type( t.numpy() ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0zj-tjXUOSO"
      },
      "source": [
        "## Autograd\n",
        "\n",
        "파이토치가 단순한 행렬 연산라이브러리가 아닌 이유는 모든 연산에 대한 추적이 가능하기 때문이다. 특히 특정 변수가 결과를 도출하는 연산을 추적해 그 변수값의 미분값을 계산해준다. 이런 기능을 autograd 라고 한다. \n",
        "\n",
        "이를 위해서 텐서에 다음과 같이 선언한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUSw6YQfDmq7"
      },
      "source": [
        "w = tc.tensor(1.0, requires_grad = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g6wj4vnVduY"
      },
      "source": [
        "이제 이를 통한 연산을 한번 만들어보도록 하겠다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3zCo6jvVcTT"
      },
      "source": [
        "y = w * 2 \n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAUDv7-VnF8"
      },
      "source": [
        "이 연산에 대한 w 의 미분은 2이다. 이것을 하기 위해 이제 역추적 명령을 실행한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rClWFM7aV_lI"
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n9equ2kWCZP"
      },
      "source": [
        "이젠 w 의 미분값을 확인해보도록 하겠다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2Gzuk6WBoY"
      },
      "source": [
        "print( w.grad )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlNEOpn2WLuf"
      },
      "source": [
        "\n",
        "이제 위의 과정을 합쳐서 좀더 복잡한 연산의 미분값을 확인해보도록 하겠다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHGITvb5WKYW"
      },
      "source": [
        "\n",
        "w = tc.tensor(1.0, requires_grad = True)\n",
        "y = 3 * w ** 2  + 2 * w\n",
        "y.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dny1F-wgYKjn"
      },
      "source": [
        "이러한 미분값이 무슨 의미가 있을까?  하는 생각이 들 것이다.  회귀분석에서 우리가 수행했던 \n",
        "\n",
        "> $ w \\times x + b $ \n",
        "\n",
        "에서 w 값을 변경할때 \n",
        "\n",
        "> $ w' = lr \\times x \\times d$\n",
        "\n",
        "라는 공식을 사용한 기억이 날 것이다. 신경망에서 찾아야 하는 답은 입력값 x 가 아니라 x 값에 대해 오차를 최소화하는 w 이다. 그리고 x 는 w 입장에서 미분 계수가 된다. sigmoid 함수에서는 sigp 라는 미분 함수를 쓴 기억이 날 것이다. 이 역시 x 에 대한 미분을 사용하는 효과가 있다. 요컨데 모든 경우 미분계수를 사용하여 오차를 줄이는 방향과 양을 결정할 수 있다. \n",
        "\n",
        "그러면 이제 이것을 활용하여 회귀문제를 풀어보도록 하겠다. 우선 전에 다뤘던 방식과 조금 차이가 필요하다. \n",
        "\n",
        "- epoch 를 한번에 처리해야 하기 때문에 오차는 제곱오차를 사용한다. \n",
        "- 미분을 사용하면 따로 d 값을 곱하지 안아도 된다. \n",
        "\n",
        "epoch 단위의 오차의 합산을 구할 시에는 음수오차와 양수오차가 서로 0을 만들 수가 있다. 이를 방지하기 위해서는 절대값이 필요한데 절대값은 미분계수를 구하기 어렵기 때문에 간편한 제곱을 사용하는 것이 좋다. \n",
        "\n",
        "또한 미분 자체가 기울기에서 d 의 크기를 어느정도 반영하게 된다. 특히 제곱오차의 경우 d 값이 너무 커서 미분계수를 아주 작게 잡지 안으면 쉽게 발산하므로 d 값 자체를 사용하지 안도록 하겠다. \n",
        "\n",
        "그리고 텐서의 값을 출력하기 위해서는 .item()을 사용해야 일반적인 실수 출력을 얻을 수 있다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wEb-qyVYI0d"
      },
      "source": [
        "import numpy as np \n",
        "xn = np.arange(10)\n",
        "yn = xn * 2 + np.random.normal(0,0.3,10)\n",
        "\n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n",
        "print(x,y)\n",
        "\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "print(w)\n",
        "\n",
        "for step in range(100):\n",
        " \n",
        "  o = x * w\n",
        "  d = (y - o).pow(2).mean() \n",
        "  print(\"{:.3f}\".format( w.item()) )\n",
        "\n",
        "  d.backward()\n",
        "  with tc.no_grad(): # 왠지 모르지만 grad 를 연산에 사용하려면 필요\n",
        "    w -= 0.01 * w.grad \n",
        "    w.grad.zero_()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqBZcSfKLp0g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRPNh2A24Q6X"
      },
      "source": [
        "### <font color = 'red'> 연습문제 : grad 사용해보기\n",
        "위의 문제에 bias 를 추가해 $ y = 2\\times x + 3$ 을 해결해보세요\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeGs3TFbLTt8"
      },
      "source": [
        "import numpy as np \n",
        "xn = np.arange(10)\n",
        "yn = xn * 2 + 3 +  np.random.normal(0,0.3,10)\n",
        "# 연습문제 코드 작성하기 \n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n",
        "print(x,y)\n",
        "\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "b = tc.rand(1, requires_grad = True)\n",
        "print(w)\n",
        "\n",
        "ds = [] \n",
        "for step in range(50):\n",
        "  \n",
        "  o = x * w + b\n",
        "  d = (y - o).pow(2).mean() \n",
        "  ds += [d.item()]\n",
        "  print(\"{:.3f} {:.3f}\".format( w.item(), b.item() ) )\n",
        "\n",
        "  d.backward()\n",
        "  with tc.no_grad(): # 왠지 모르지만 grad 를 연산에 사용하려면 필요\n",
        "    w -= 0.01 * w.grad \n",
        "    b -= 0.01 * b.grad \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZkZZxlkXiVc"
      },
      "source": [
        "## Optimizer \n",
        " \n",
        "autograd 는 오차값을 최소화시켜주는 w의 변경값을 위한 미분값을 찾아주지만 그럼에도 신경망이 복잡하고 계층이 깊어지면 이런 모든 작업을 직접 하는 것은 상당히 골치아픈 일이다.  다행히 torch 에는 이런 과정을 자동화하는 기능을 지원한다.  이를 Optimizer 라고 한다. \n",
        "\n",
        "이를 위해서는 다음 서브라이브러리를 호출해야한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7BEQFTWik-"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN0Z8mWmlPL9"
      },
      "source": [
        "그리고 먼저 optimizer 를 생성한다. 여기서는 가장 기본적은 경사하강법(SGD) 을 사용하도록 하겠다.  이때 첫번째 인자로는 최적화 변수들을 선택해야 한다. 이 변수들은 반드시 requires_grad 설정이 되어있어야 한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFYZw1rbkkh0"
      },
      "source": [
        "optimizer = optim.SGD([w], lr = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-upY5s7mhne"
      },
      "source": [
        "학습은 다음과 같은 과정으로 이뤄진다. \n",
        "\n",
        "- 오차 계산\n",
        "- optimizer.zero_grad() : grad 초기화 \n",
        "- d.backward()  : grad 계산\n",
        "- optimizer.step() : w 값 업데이트\n",
        "\n",
        "그러면 위의 문제를 optimizer 를 사용해 풀어보도록 하겠다. 먼저 x,y 를 만들고 w 를 초기화 한 후에 optimizer 를 생성한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsCZyDnBnVQ2"
      },
      "source": [
        "# 데이터 초기화 \n",
        "xn = np.arange(10)\n",
        "yn = xn * 2 + np.random.normal(0,0.3,10)\n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizer = optim.SGD([w], lr = 0.01)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjKcxSton9Oi"
      },
      "source": [
        "이제 다음의 과정을 반복한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhE45lgnXbt"
      },
      "source": [
        "o = w * x \n",
        "d = ( y - o ).mean() \n",
        "\n",
        "optimizer.zero_grad()\n",
        "d.backward()\n",
        "optimizer.step() \n",
        "\n",
        "print(\"w:{:.3f} err:{:.3f}\".format(w.item(), d.item()) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI5Cl28goltl"
      },
      "source": [
        "? 어 그런데 반복할수록 오히려 w 가 기하급수적으로 커지고 큰 수치로 마이너스로 떨어지며 수렴하지 안는것을 볼 수 있을것이다.  이는 \n",
        "\n",
        ">  경사하강법은 오차를 최소화하는 방향으로 작동한다. \n",
        "\n",
        "요컨데 - 라도 최소화면 가능하면 그 방향으로 움직인다는 것이다.  이를 해결하기 위해서는 d 를 계산할때 제곱을 해주면 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll2lyLO4oc3m"
      },
      "source": [
        "o = w * x \n",
        "d = ( y - o ).pow(2).mean() \n",
        "\n",
        "optimizer.zero_grad()\n",
        "d.backward()\n",
        "optimizer.step() \n",
        "\n",
        "print(\"w:{:.3f} err:{:.3f}\".format(w.item(), d.item()) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prfqpLPkQy39"
      },
      "source": [
        "### <font color = 'red'> 연습문제 : Optimizer 활용\n",
        "위의 예를 반복해서 d 의 값이 0.01 이하로 떨어질때까지 작동하는 반복문을 작성하고  해당 step을 출력하세요\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FTVh0thpPsF"
      },
      "source": [
        "xn = np.arange(10)\n",
        "yn = xn * 2 + np.random.normal(0,0.3,10)\n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "\n",
        "optimizer = optim.SGD([w], lr = 0.01)\n",
        "\n",
        "## 연습문제의 코드를 작성하세요\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qhba2r9RT97"
      },
      "source": [
        "### <font color = 'red'> 연습문제 : Optimizer 에 bias 적용\n",
        "이번에는 bias 를 적용해보세요\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewz_T6G-RT-F"
      },
      "source": [
        "xn = np.arange(10)\n",
        "yn = xn * 4 - 3 + np.random.normal(0,0.3,10)\n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "\n",
        "## 연습문제의 코드를 작성하세요\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}