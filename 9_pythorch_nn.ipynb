{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_pythorch_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR19aRfL28bT5ViHi8HWQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoz/mlclass/blob/master/9_pythorch_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F93k2IjJl3Pr"
      },
      "source": [
        "import torch as tc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwTqjKtp5Rl3"
      },
      "source": [
        "# NN 모듈 \n",
        "\n",
        "앞에서는 Autograd 와 Optimizer 를 활용해 간단한 회귀문제를 해결하는 방법을 보았다.  Optimizer 를 사용하면 일일이 웨이트와 바이어스 값을 업데이트 해야하는 과정이 단순화되지만 그럼에도 다층 신경망처럼 복잡한 구조의 신경망에서는 다차원 배열로 전파과정을 만들어주는 것이 초보자에겐 쉽지 안다. \n",
        "\n",
        "이런 번거로움을 줄이기 위해서 입력값-> 출력값의 계산과정을 단순화 시켜줄 수 있게 많이 사용하는 신경망의 구조와 계산을 클래스화 시켜둔 것이 nn 라이브러리이다. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06yXUtZE6pwg"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRZoIixs6vkH"
      },
      "source": [
        "## NN 모듈로 단순 선형회귀 제작\n",
        "\n",
        "그러면 이제 nn모듈이 어떤 식으로 이 과정을 단순화 시켰는지 한번 알아보자. 단순 선형회귀 문제를 bias 까지 포함해 한번 보도록 하겠다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uIXVmQ96ueO"
      },
      "source": [
        "# 데이터 초기화 \n",
        "xn = np.arange(10).reshape(-1,1)\n",
        "yn = xn * 2 + 4 + np.random.normal(0,0.3,10)\n",
        "\n",
        "x = tc.FloatTensor(xn)\n",
        "y = tc.FloatTensor(yn)\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh6bq01J7d6Y"
      },
      "source": [
        "### 기존의 선형회귀 방식 \n",
        "\n",
        "먼저 이를 위해 이전챕터에서 구현한 단순 선형회귀 모델을 먼저 보도록 하겠다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_OSR2KH7aiy",
        "outputId": "8a336692-2e4e-45a6-8ddf-7230bdc19c84"
      },
      "source": [
        "# 파라미터 생성\n",
        "w = tc.rand(1, requires_grad = True)\n",
        "b = tc.rand(1, requires_grad = True)\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizer = optim.SGD([w,b], lr = 0.01)\n",
        "\n",
        "for step in range(1000):\n",
        "  o = w * x + b  # 출력 계산\n",
        "  d = ( y - o ).pow(2).mean() # 에러 계산\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  d.backward()\n",
        "  optimizer.step() \n",
        "\n",
        "  if step % 100 == 0 : \n",
        "    print(\"w:{:.3f} err:{:.3f}\".format(w.item(), d.item()) )\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w:1.741 err:91.382\n",
            "w:2.254 err:0.892\n",
            "w:2.139 err:0.339\n",
            "w:2.073 err:0.161\n",
            "w:2.036 err:0.103\n",
            "w:2.015 err:0.085\n",
            "w:2.003 err:0.079\n",
            "w:1.996 err:0.077\n",
            "w:1.993 err:0.077\n",
            "w:1.990 err:0.076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOjCWofL7y9i"
      },
      "source": [
        "### NN 모듈을 이용한 선형회귀\n",
        "\n",
        "NN 모듈은 다음의 세가지를 지원해준다. \n",
        "\n",
        "> - weight 및 bias 생성\n",
        "> - output 계산과정\n",
        "> - 오차값(d) 계산 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oF6750X9isT"
      },
      "source": [
        "\n",
        "\n",
        "#### 모델 생성\n",
        "\n",
        "위의 코드가 어떻게 바뀌는 지 보도록 한다. 우선 네트워크의 형태를 모델로 정의한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oAoKFsA9oID"
      },
      "source": [
        "model = nn.Linear(1,1)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqvfzCDo9tEL"
      },
      "source": [
        "이 과정을 통해 weight 값과 bias 값이 생성된다.  다음 코드는 model 이 자동생성해준 weight 와 bias 값을 보여준다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R11sY33i95Jy",
        "outputId": "7eb24572-946f-43b1-c766-04704a47ff9d"
      },
      "source": [
        "print( list( model.parameters()) )"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.4548]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.1508], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8PiZmIoAu5k"
      },
      "source": [
        "#### 파라미터 등록\n",
        "\n",
        "예전에 w,b 를 직접 만든 경우는  \n",
        "```\n",
        "optimizer = optim.SGD([w,b], lr = 0.1)\n",
        "```\n",
        "이런 식으로 w,b 를 등록해서 사용했다. nn.model 도 마찬가지로 model.parameters() 값을 등록해주면 된다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nckmP0nXBaoz"
      },
      "source": [
        "optimzer = optim.SGD( model.parameters(), lr = 0.1)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNMmO0q3-Eak"
      },
      "source": [
        "#### 출력값 계산 \n",
        "\n",
        "이제 다음과정을 통해 간단히 출력값을 계산할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXbvn3Bp-Q2b",
        "outputId": "7f5f17ab-37a3-4066-9ea1-0b66e54d5bf1"
      },
      "source": [
        "o = model(x)\n",
        "print(o)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1508],\n",
            "        [-0.6056],\n",
            "        [-1.0604],\n",
            "        [-1.5153],\n",
            "        [-1.9701],\n",
            "        [-2.4249],\n",
            "        [-2.8797],\n",
            "        [-3.3346],\n",
            "        [-3.7894],\n",
            "        [-4.2442]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5woQoKBv-SAu"
      },
      "source": [
        "\n",
        "#### 에러(코스트) 계산 \n",
        "\n",
        "이전에 (y-o).pow(2).mean() 구문을 기억하는가? 차이값을 제곱해서 평균을 냈었다. 이것을 nn모듈에선 mean square error 라고 하며 다음과 같이 d 값을 계산한다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5VhX4Sy_Z9b",
        "outputId": "0a7d8240-bca0-4059-e3bb-b2e358298ef2"
      },
      "source": [
        "d = nn.functional.mse_loss(o, y)\n",
        "print(d)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(280.7339, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Using a target size (torch.Size([10, 10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd-Iwnca_55u"
      },
      "source": [
        "보면 텐서 연산이 필요한 부분을 대부분 감추고 함수화시켜둔 것을 알 수 있다. 이제 위의 내용을 적용해 전체 선형회귀 과정을 보도록 하자. \n",
        "\n",
        "\n",
        "#### 전체 과정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLaTiadGAYT_",
        "outputId": "b78ce391-32f5-4c0c-8ab7-8f8d9927bd08"
      },
      "source": [
        "# 모델 생성\n",
        "model = nn.Linear(1,1)\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "for step in range(1000):\n",
        "  o = model(x)   # 출력 계산\n",
        "  d = nn.functional.mse_loss(o,y) # 에러 계산\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  d.backward()\n",
        "  optimizer.step() \n",
        "\n",
        "  if step % 100 == 0 : \n",
        "    print(\"w:{:.3f} err:{:.3f}\".format(w.item(), d.item()) )\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Using a target size (torch.Size([10, 10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "w:1.989 err:173.395\n",
            "w:1.989 err:1.020\n",
            "w:1.989 err:0.373\n",
            "w:1.989 err:0.164\n",
            "w:1.989 err:0.097\n",
            "w:1.989 err:0.075\n",
            "w:1.989 err:0.068\n",
            "w:1.989 err:0.066\n",
            "w:1.989 err:0.066\n",
            "w:1.989 err:0.065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mBpnyIxtvE1"
      },
      "source": [
        "## 다중 선형 회귀\n",
        "\n",
        "이번에는 다중 선형회귀 모델을 직접 구현한 것과 NN 모델을 사용한 것을 비교해보도록 하겠다.  먼저 다중선형회귀 문제를 정의한다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohj_oFvcCZQN"
      },
      "source": [
        "x1 = np.random.random(10)\n",
        "x2 = np.random.random(10)\n",
        "\n",
        "yn = 4 * x1 + 2 * x2 + 7\n",
        "\n",
        "x = tc.FloatTensor(np.c_[x1,x2])\n",
        "y = tc.FloatTensor(yn.reshape(-1,1))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRA-Zt-aCyPa"
      },
      "source": [
        "### Optimizer 만 사용\n",
        "\n",
        "다중 선형회귀의 경우 다른것은 크게 신경쓰지 안아도 된다. 다만 \n",
        "```\n",
        "o  = ( x * w ).sum(dim=1) + b \n",
        "```\n",
        "는 결과가 1차원이 나온다. 이 경우 위의 y 값은 2차원 (-1,1) 형태이므로 이를 다음과 같이 세워준다. \n",
        "```\n",
        "o  = (( x * w ).sum(dim=1) + b).view(-1,1)\n",
        "```\n",
        "그러면 이제 코드를 보도록 하자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5OEPvSMl5TD",
        "outputId": "77a604b0-97bd-434f-bbd7-18010d570770"
      },
      "source": [
        "w = tc.rand(2, requires_grad = True )\n",
        "b = tc.rand(1, requires_grad = True )\n",
        "\n",
        "op = optim.SGD([w, b], lr = 0.05)\n",
        "\n",
        "ds = []\n",
        "for step in range(1000):\n",
        "  o = ((x * w).sum(dim = 1) + b).view(-1,1)\n",
        "  d = (y - o).pow(2).mean()\n",
        "\n",
        "  op.zero_grad()\n",
        "  d.backward()\n",
        "  op.step()\n",
        "  \n",
        "  if step % 100 == 0 :\n",
        "    ds += [d] \n",
        "    print( \"w:{}, b:{:.3f} \\t err:{:.3f}\".format( w.detach().numpy(), b.item() , d ) )\n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w:[1.2208909  0.76178956], b:1.418 \t err:65.035\n",
            "w:[3.3061008 2.650201 ], b:6.874 \t err:0.063\n",
            "w:[3.607381  2.2986987], b:6.964 \t err:0.016\n",
            "w:[3.7805674 2.14083  ], b:6.994 \t err:0.004\n",
            "w:[3.878385  2.0680768], b:7.002 \t err:0.001\n",
            "w:[3.932989  2.0336673], b:7.003 \t err:0.000\n",
            "w:[3.963228 2.016987], b:7.002 \t err:0.000\n",
            "w:[3.979879  2.0087135], b:7.002 \t err:0.000\n",
            "w:[3.9890127 2.0045311], b:7.001 \t err:0.000\n",
            "w:[3.9940093 2.0023806], b:7.001 \t err:0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDtlvAStGYzY"
      },
      "source": [
        "### nn을 사용\n",
        "\n",
        "nn의 경우는 단지 입력값의 차원만 다음과 같이 바꿔주면 된다 \n",
        "```\n",
        "model = nn.Linear(2,1) \n",
        "```\n",
        "이렇게 하면 나머지 출력계산이 자동화된다. 단순 선형회귀가 사실 nn모델을 사용하는데 크게 편리함을 느끼기 어려웠던 것에 비해 이런 부분은 확실히 그 편리함을 느낄 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0tXLSNLE3Vh",
        "outputId": "dc8e5650-c9b9-488f-8052-ae76db9cee70"
      },
      "source": [
        "\n",
        "model = nn.Linear( 2, 1 )\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "ds = []\n",
        "for step in range(1000):\n",
        "  o = model(x)\n",
        "  d = nn.functional.mse_loss(o, y)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  d.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "\n",
        "  if step % 50 == 0 :\n",
        "    ds += [d.item()]\n",
        "    print(\"err:\", d.item())\n",
        "\n",
        "print(list( model.parameters()) )"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "err: 70.05973815917969\n",
            "err: 4.999509334564209\n",
            "err: 0.5519677996635437\n",
            "err: 0.2220471352338791\n",
            "err: 0.17540141940116882\n",
            "err: 0.1512843668460846\n",
            "err: 0.13153450191020966\n",
            "err: 0.11451970040798187\n",
            "err: 0.09978832304477692\n",
            "err: 0.08701812475919724\n",
            "err: 0.07593761384487152\n",
            "err: 0.06631498783826828\n",
            "err: 0.05795113369822502\n",
            "err: 0.0506754033267498\n",
            "err: 0.0443408265709877\n",
            "err: 0.03882177546620369\n",
            "err: 0.03400922194123268\n",
            "err: 0.02980991080403328\n",
            "err: 0.02614297904074192\n",
            "err: 0.022938739508390427\n",
            "[Parameter containing:\n",
            "tensor([[3.5377, 2.3192]], requires_grad=True), Parameter containing:\n",
            "tensor([6.9749], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TND-PHL1HTDk"
      },
      "source": [
        "## 로지스틱 회귀\n",
        "\n",
        "이번에는 and 문제를 optimizer 만 사용해 구현한 후에 이것이 nn 에선 어떻게 구현되는 지를 비교해보도록 하겠다. 우선 and 문제를 정의하자. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjCb9ThIIdQD"
      },
      "source": [
        "x = tc.FloatTensor( [[0,0],[0,1],[1,0],[1,1]] )\n",
        "y = tc.FloatTensor( [[0],[0],[0],[1]])\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Px5tozmIn1s"
      },
      "source": [
        "\n",
        "### Optimizer 만 사용\n",
        "\n",
        "0과 1의 범주형 데이터를 예측하는 로지스틱 회귀문제는 Optimizer 없이 구현시 직접 sigp() 라는 시그모이드의 미분 함수를 제작해서 오차값에 적용시켰다. 하지만 Optimizer 와 autograd 는 이러한 함수에 대한 미분이 자동계산 되므로 이런 과정이 필요없다. \n",
        "\n",
        "로지스틱 회귀의 경우 위에 사용된 다중회귀에 sigmoid만 작용시켜면 간단히 해결된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFXkXI_aIzrc",
        "outputId": "5b9a2710-4bc7-4111-df8d-a1c12d1cfdf5"
      },
      "source": [
        "w = tc.rand(2, requires_grad = True )\n",
        "b = tc.rand(1, requires_grad = True )\n",
        "\n",
        "op = optim.SGD([w, b], lr = 0.1)\n",
        "\n",
        "ds = []\n",
        "for step in range(10000):\n",
        "  o = ((x * w).sum(dim = 1) + b).sigmoid().view(-1,1)\n",
        "  d = (y - o).pow(2).mean()\n",
        "\n",
        "  op.zero_grad()\n",
        "  d.backward()\n",
        "  op.step()\n",
        "  \n",
        "  if step % 1000 == 0 :\n",
        "    ds += [d] \n",
        "    print( \"err:{:.3f}\".format( d ) )\n",
        "\n",
        "print(np.round( o.detach().numpy(), 3) )"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "err:0.480\n",
            "err:0.059\n",
            "err:0.032\n",
            "err:0.022\n",
            "err:0.016\n",
            "err:0.012\n",
            "err:0.010\n",
            "err:0.009\n",
            "err:0.007\n",
            "err:0.006\n",
            "[[0.001]\n",
            " [0.082]\n",
            " [0.082]\n",
            " [0.902]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hLB_OPfKgma"
      },
      "source": [
        "### nn 모델을 사용 \n",
        "\n",
        "nn모델의 경우 sigmoid 도 하나의 layer 로 정의한다. 그렇기 때문에 model.Linear 에 추가로 하나의 층을 붙여줘야한다. 이를 위해서 여러 층을 하나의 모델로 합치기 위해서는위해 nn.Sequential() 이라는 모델을 사용한다. \n",
        "\n",
        "```\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(2,1),\n",
        "  nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "이 점만 신경쓰면 nn 모델 역시 다른 부분의 수정 없이 구현이 가능하다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q7qw9VvMyDo",
        "outputId": "4c64875b-df30-4061-9bf8-2c41ab5cb73f"
      },
      "source": [
        "model = nn.Sequential(\n",
        "     nn.Linear(2,1),\n",
        "     nn.Sigmoid()\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.5)\n",
        "\n",
        "for step in range(10000):\n",
        "  o = model(x)\n",
        "\n",
        "  d = nn.functional.mse_loss(o, y)\n",
        "  optimizer.zero_grad() \n",
        "  d.backward()\n",
        "  optimizer.step() \n",
        "\n",
        "  if step % 500 == 0 :\n",
        "    print(\"err:\", d.item())\n",
        "\n",
        "\n",
        "print( np.round(o.detach().numpy(), 3) )\n"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5166, 0.0166]], requires_grad=True), Parameter containing:\n",
            "tensor([0.4703], requires_grad=True)]\n",
            "err: 0.34126701951026917\n",
            "err: 0.026026133447885513\n",
            "err: 0.01240517757833004\n",
            "err: 0.007894625887274742\n",
            "err: 0.005720104090869427\n",
            "err: 0.00445788586512208\n",
            "err: 0.0036393660120666027\n",
            "err: 0.00306803360581398\n",
            "err: 0.002647782675921917\n",
            "err: 0.0023263031616806984\n",
            "err: 0.0020727980881929398\n",
            "err: 0.0018679799977689981\n",
            "err: 0.0016991941956803203\n",
            "err: 0.0015577899757772684\n",
            "err: 0.0014376681065186858\n",
            "err: 0.0013344006147235632\n",
            "err: 0.001244708662852645\n",
            "err: 0.0011660999152809381\n",
            "err: 0.0010966607369482517\n",
            "err: 0.001034886110574007\n",
            "[[0.   ]\n",
            " [0.034]\n",
            " [0.034]\n",
            " [0.96 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS9u3ATuuBK2"
      },
      "source": [
        "## 다층 신경망\n",
        "\n",
        "이제 xor 문제를 풀기 위한 다층 신경망을 구현해보자. 우선 문제 정의는 다음과 같다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLGdKXiOBoB"
      },
      "source": [
        "\n",
        "x = tc.FloatTensor( [[0,0],[0,1],[1,0],[1,1]] )\n",
        "y = tc.FloatTensor( [[0],[1],[1],[0]])\n"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNs2EJ2SOIl7"
      },
      "source": [
        "### Optimizer만 사용\n",
        "\n",
        "Optimizer 만 사용하더라도 다층 신경망의 경우 여러개의 다차원 웨이트와 히든을 바이어스를 갖기 때문에 그 계산이 상당히 복잡하다. 일단 히든 노드는 2개만 사용하도록 하겠다. 이 경우 구조는 2 x 2 x 1 이 되며 \n",
        "```\n",
        "w1 -> 2 x 2 \n",
        "b1 -> 2 \n",
        "w2 -> 2\n",
        "b2 -> 1\n",
        "```\n",
        "이렇게 구성된다. \n",
        "\n",
        "히든에서 출력까지는 위에서 했던 단층 로지스틱회귀와 유사하므로 크게 신경쓰지 안아도 되나 2 x 2 의 w1 을 이용해 h 를 계산하는 부분이 상당히 복잡한데 결론부터 이야기하면 \n",
        "\n",
        "```\n",
        "h = ( (x.view(4,1,2) * w1).sum(dim = 2) + b1 ).sigmoid()\n",
        "```\n",
        "로 계산된다.  그리고 xor 문제는 평균적으로 많은 스탭을 필요로 하는 것으로 알려져 있지만 대신 학습률(lr)을 상당히 높여도 발산하지 안으므로 lr 은 1 로 주도록 하겠다.  하지만 그럼에도 불구하고 상당히 많은 스탭을 필요로 하는 것을 알 수 있을 것이다. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inN0mx_1O20V",
        "outputId": "c151757c-fb40-4867-8e7f-62d872049852"
      },
      "source": [
        "w1 = tc.rand([2,2], requires_grad = True)\n",
        "w2 = tc.rand(2, requires_grad = True)\n",
        "\n",
        "b1 = tc.rand(2, requires_grad = True)\n",
        "b2 = tc.rand(1, requires_grad = True)\n",
        "\n",
        "op = optim.SGD([w1, w2, b1, b2], lr = 1)\n",
        "ds = []\n",
        "for step in range(10000):     \n",
        "  h = ( (x.view(4,1,2) * w1).sum(dim = 2) + b1 ).sigmoid()\n",
        " \n",
        "  o = ( (h * w2).sum( dim = 1) + b2 ).sigmoid().view(-1,1)\n",
        "  d = (y - o ).pow(2).mean()\n",
        "\n",
        "  op.zero_grad() \n",
        "  d.backward() \n",
        "  op.step() \n",
        "    \n",
        "  if step % 100 == 0 :    \n",
        "    print(\"err:\", d.item())\n",
        "\n",
        "    ds += [d.item()]\n",
        "    \n",
        "    \n",
        "print( \"o:{}, \\t err:{:.3f}\".format( o.detach().numpy(), d ) )\n"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "err: 0.3167257010936737\n",
            "err: 0.24885056912899017\n",
            "err: 0.24660149216651917\n",
            "err: 0.24000650644302368\n",
            "err: 0.22517642378807068\n",
            "err: 0.2053285837173462\n",
            "err: 0.19069835543632507\n",
            "err: 0.18067558109760284\n",
            "err: 0.16999578475952148\n",
            "err: 0.15209625661373138\n",
            "err: 0.11709535121917725\n",
            "err: 0.07413457334041595\n",
            "err: 0.045340441167354584\n",
            "err: 0.03000437095761299\n",
            "err: 0.021550791338086128\n",
            "err: 0.016470063477754593\n",
            "err: 0.013169577345252037\n",
            "err: 0.010888993740081787\n",
            "err: 0.009235197678208351\n",
            "err: 0.007989284582436085\n",
            "err: 0.007021475117653608\n",
            "err: 0.006250652484595776\n",
            "err: 0.005623889155685902\n",
            "err: 0.005105284973978996\n",
            "err: 0.004669790156185627\n",
            "err: 0.004299373831599951\n",
            "err: 0.003980806563049555\n",
            "err: 0.0037041716277599335\n",
            "err: 0.003461873158812523\n",
            "err: 0.0032480342779308558\n",
            "err: 0.003058014437556267\n",
            "err: 0.0028881344478577375\n",
            "err: 0.0027354236226528883\n",
            "err: 0.002597440965473652\n",
            "err: 0.002472200896590948\n",
            "err: 0.0023580524139106274\n",
            "err: 0.002253602957352996\n",
            "err: 0.0021576974540948868\n",
            "err: 0.0020693400874733925\n",
            "err: 0.001987691270187497\n",
            "err: 0.001912032370455563\n",
            "err: 0.0018417327664792538\n",
            "err: 0.0017762533389031887\n",
            "err: 0.0017151241190731525\n",
            "err: 0.001657932880334556\n",
            "err: 0.0016043137293308973\n",
            "err: 0.0015539481537416577\n",
            "err: 0.0015065567567944527\n",
            "err: 0.0014618863351643085\n",
            "err: 0.0014197067357599735\n",
            "err: 0.001379828667268157\n",
            "err: 0.001342062489129603\n",
            "err: 0.0013062504585832357\n",
            "err: 0.0012722467072308064\n",
            "err: 0.0012399203842505813\n",
            "err: 0.0012091536773368716\n",
            "err: 0.0011798302875831723\n",
            "err: 0.0011518634855747223\n",
            "err: 0.0011251562973484397\n",
            "err: 0.001099626300856471\n",
            "err: 0.001075197709724307\n",
            "err: 0.001051805098541081\n",
            "err: 0.0010293793166056275\n",
            "err: 0.0010078689083456993\n",
            "err: 0.000987216830253601\n",
            "err: 0.0009673720924183726\n",
            "err: 0.000948292319662869\n",
            "err: 0.0009299288503825665\n",
            "err: 0.0009122468763962388\n",
            "err: 0.0008952098432928324\n",
            "err: 0.0008787818951532245\n",
            "err: 0.0008629325311630964\n",
            "err: 0.000847632298246026\n",
            "err: 0.0008328488911502063\n",
            "err: 0.0008185638580471277\n",
            "err: 0.0008047478622756898\n",
            "err: 0.0007913816953077912\n",
            "err: 0.0007784415502101183\n",
            "err: 0.0007659060647711158\n",
            "err: 0.0007537606870755553\n",
            "err: 0.0007419880712404847\n",
            "err: 0.0007305655162781477\n",
            "err: 0.0007194839417934418\n",
            "err: 0.000708727864548564\n",
            "err: 0.0006982794729992747\n",
            "err: 0.0006881284061819315\n",
            "err: 0.0006782637210562825\n",
            "err: 0.0006686680717393756\n",
            "err: 0.0006593368598259985\n",
            "err: 0.000650256872177124\n",
            "err: 0.0006414225790649652\n",
            "err: 0.0006328159943223\n",
            "err: 0.000624430482275784\n",
            "err: 0.0006162635982036591\n",
            "err: 0.0006083017797209322\n",
            "err: 0.0006005385657772422\n",
            "err: 0.0005929674953222275\n",
            "err: 0.0005855817580595613\n",
            "err: 0.0005783732049167156\n",
            "err: 0.0005713377031497657\n",
            "o:[[0.02490852]\n",
            " [0.9781633 ]\n",
            " [0.97815984]\n",
            " [0.02615086]], \t err:0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCcjUQb-Rmju"
      },
      "source": [
        "도식화 시켜보면 상당히 인상적인 곡선을 볼 수 있는데 일반적인 학습과 달리 에러의 하강이 느슨해지다가 일정시점에서 급격히 떨어지는 폭포수 형태를 볼 수 있을 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "VmgWWqKZRlvv",
        "outputId": "b4f2142d-5873-4948-b170-43c339ae0c98"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt \n",
        "plt.plot(ds)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f05aa715450>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRnC7VFsR-HO"
      },
      "source": [
        "### nn 모델을 사용\n",
        "\n",
        "위의 방법과 달리 nn모델을 사용하면 위의 복잡한 웨이트 초기화와 계산과정은 다 자동화된다. 단지 다음과 같이 모델을 구성할때 2개의 층만 더 추가해주면 된다. \n",
        "```\n",
        "model = nn.Sequential(\n",
        "     nn.Linear(2,2),\n",
        "     nn.Sigmoid(),\n",
        "     nn.Linear(2,1),\n",
        "     nn.Sigmoid()\n",
        ")\n",
        "\n",
        "```\n",
        "이를 이용한 웨이트와 바이어스 생성, 출력 계산등은 다 자동화 되므로 사실상 이 부분을 제외하면 나머지는 단층 신경망과 동일하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qybt3pwIm2ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f57ccd6-2b24-4382-e169-f794e19456f6"
      },
      "source": [
        "model = nn.Sequential(\n",
        "     nn.Linear(2,2),\n",
        "     nn.Sigmoid(),\n",
        "     nn.Linear(2,1),\n",
        "     nn.Sigmoid()\n",
        ")\n",
        " \n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1)\n",
        " \n",
        "ds = []\n",
        "for step in range(10000):\n",
        "  o = model(x)\n",
        "\n",
        "  d = nn.functional.mse_loss(o, y)\n",
        "  optimizer.zero_grad() \n",
        "  d.backward()\n",
        "  optimizer.step() \n",
        "\n",
        "\n",
        "  if step % 100 == 0 :\n",
        "    ds += [d.item()]\n",
        "    print(\"err:\", d.item())\n",
        "\n",
        "\n",
        "print( np.round(o.detach().numpy(), 3) )\n",
        "\n",
        "\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "err: 0.26125481724739075\n",
            "err: 0.24957376718521118\n",
            "err: 0.24914570152759552\n",
            "err: 0.248040571808815\n",
            "err: 0.24459823966026306\n",
            "err: 0.23366297781467438\n",
            "err: 0.21155884861946106\n",
            "err: 0.18704096972942352\n",
            "err: 0.14411164820194244\n",
            "err: 0.06783607602119446\n",
            "err: 0.030713152140378952\n",
            "err: 0.017709234729409218\n",
            "err: 0.011953971348702908\n",
            "err: 0.008859161287546158\n",
            "err: 0.006968081463128328\n",
            "err: 0.005707951728254557\n",
            "err: 0.004814734682440758\n",
            "err: 0.004151804838329554\n",
            "err: 0.0036420286633074284\n",
            "err: 0.003238857723772526\n",
            "err: 0.00291265151463449\n",
            "err: 0.0026436815969645977\n",
            "err: 0.0024183797650039196\n",
            "err: 0.0022270968183875084\n",
            "err: 0.0020627963822335005\n",
            "err: 0.0019202433759346604\n",
            "err: 0.0017954586073756218\n",
            "err: 0.0016853694105520844\n",
            "err: 0.0015875614481046796\n",
            "err: 0.0015001248102635145\n",
            "err: 0.0014215154806151986\n",
            "err: 0.001350484904833138\n",
            "err: 0.0012859979178756475\n",
            "err: 0.00122720783110708\n",
            "err: 0.0011733982246369123\n",
            "err: 0.0011239750310778618\n",
            "err: 0.00107842858415097\n",
            "err: 0.001036326284520328\n",
            "err: 0.0009972968837246299\n",
            "err: 0.0009610196575522423\n",
            "err: 0.000927216315176338\n",
            "err: 0.0008956483798101544\n",
            "err: 0.0008661036845296621\n",
            "err: 0.0008383943350054324\n",
            "err: 0.000812356942333281\n",
            "err: 0.0007878473261371255\n",
            "err: 0.0007647349848411977\n",
            "err: 0.000742907403036952\n",
            "err: 0.0007222623680718243\n",
            "err: 0.0007027014507912099\n",
            "err: 0.0006841488648205996\n",
            "err: 0.0006665281252935529\n",
            "err: 0.0006497717113234103\n",
            "err: 0.0006338151288218796\n",
            "err: 0.0006186061655171216\n",
            "err: 0.0006040929583832622\n",
            "err: 0.0005902301636524498\n",
            "err: 0.0005769734852947295\n",
            "err: 0.0005642889882437885\n",
            "err: 0.0005521333077922463\n",
            "err: 0.000540482229553163\n",
            "err: 0.0005292997811920941\n",
            "err: 0.000518561399076134\n",
            "err: 0.0005082417046651244\n",
            "err: 0.0004983128746971488\n",
            "err: 0.0004887605318799615\n",
            "err: 0.0004795560089405626\n",
            "err: 0.0004706875479314476\n",
            "err: 0.0004621330590452999\n",
            "err: 0.0004538766806945205\n",
            "err: 0.0004459049378056079\n",
            "err: 0.0004382012411952019\n",
            "err: 0.00043075604480691254\n",
            "err: 0.00042355427285656333\n",
            "err: 0.000416584312915802\n",
            "err: 0.0004098338831681758\n",
            "err: 0.0004032959113828838\n",
            "err: 0.00039695907616987824\n",
            "err: 0.0003908131620846689\n",
            "err: 0.0003848523192573339\n",
            "err: 0.0003790660994127393\n",
            "err: 0.0003734480415005237\n",
            "err: 0.0003679920337162912\n",
            "err: 0.0003626886464189738\n",
            "err: 0.0003575344744604081\n",
            "err: 0.00035252218367531896\n",
            "err: 0.00034764473093673587\n",
            "err: 0.000342898623785004\n",
            "err: 0.0003382769355084747\n",
            "err: 0.00033377629006281495\n",
            "err: 0.0003293926711194217\n",
            "err: 0.00032511967583559453\n",
            "err: 0.0003209541901014745\n",
            "err: 0.00031689164461568\n",
            "err: 0.00031293026404455304\n",
            "err: 0.00030906361644156277\n",
            "err: 0.00030529047944583\n",
            "err: 0.000301607622532174\n",
            "err: 0.00029800768243148923\n",
            "err: 0.00029449348221533\n",
            "[[0.017]\n",
            " [0.98 ]\n",
            " [0.984]\n",
            " [0.015]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GrbU8VlaTR5-",
        "outputId": "cd72c084-371d-4ede-c1ad-f5ddf5f2c63d"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt \n",
        "plt.plot(ds)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f05aa488f50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}